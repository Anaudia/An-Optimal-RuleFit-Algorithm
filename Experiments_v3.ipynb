{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7306a81b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:This copy of Interpretable AI software is for academic purposes only and not for commercial use.\n"
     ]
    }
   ],
   "source": [
    "from pauls_functions_advanced_v3 import *\n",
    "from experiment_functions import *\n",
    "import pandas as pd\n",
    "from pmlb import fetch_data, classification_dataset_names\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "classification_dataset_names = classification_dataset_names[1:10]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from joblib import Parallel\n",
    "\n",
    "class ProgressParallel(Parallel):\n",
    "    def __init__(self, use_tqdm=True, total=None, *args, **kwargs):\n",
    "        self._use_tqdm = use_tqdm\n",
    "        self._total = total\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        with tqdm(disable=not self._use_tqdm, total=self._total) as self._pbar:\n",
    "            return Parallel.__call__(self, *args, **kwargs)\n",
    "\n",
    "    def print_progress(self):\n",
    "        if self._total is None:\n",
    "            self._pbar.total = self.n_dispatched_tasks\n",
    "        self._pbar.n = self.n_completed_tasks\n",
    "        self._pbar.refresh()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def get_feature_type(x, include_binary=False):\n",
    "    x.dropna(inplace=True)\n",
    "    if not check_if_all_integers(x):\n",
    "        return 'continuous'\n",
    "    else:\n",
    "        if x.nunique() > 10:\n",
    "            return 'continuous'\n",
    "        if include_binary:\n",
    "            if x.nunique() == 2:\n",
    "                return 'binary'\n",
    "        return 'categorical'\n",
    "\n",
    "def get_target_type(x, include_binary=False):\n",
    "    x.dropna(inplace=True)\n",
    "    if x.dtype=='float64':\n",
    "        return 'continuous'\n",
    "    elif x.dtype=='int64':\n",
    "        if include_binary:\n",
    "            if x.nunique() == 2:\n",
    "                return 'binary'\n",
    "        return 'categorical'\n",
    "    else:\n",
    "        raise ValueError(\"Error getting type\")\n",
    "\n",
    "def check_if_all_integers(x):\n",
    "    \"check a pandas.Series is made of all integers.\"\n",
    "    return all(float(i).is_integer() for i in x.unique())\n",
    "def corr_data_for(df):\n",
    "    TARGET_NAME = 'target'\n",
    "    feat_names = [col for col in df.columns if col!=TARGET_NAME]\n",
    "    types = [get_feature_type(df[col], include_binary=True) for col in feat_names]\n",
    "    col = pd.DataFrame(feat_names,types)\n",
    "    num_col = col[col.index == 'continuous']\n",
    "    bin_col = col[col.index == 'binary']\n",
    "    cat_col = col[col.index == 'categorical']\n",
    "    cat_col = cat_col[0].tolist()\n",
    "    dummy_col = pd.get_dummies(data=df, columns=cat_col)\n",
    "    add_col = dummy_col.shape[1] - df.shape[1]\n",
    "    if (add_col < df.shape[0] *0.3) & (dummy_col.shape[1] <  df.shape[0]):\n",
    "        df = dummy_col\n",
    "        df.columns = df.columns.str.replace('.','_',regex=True)\n",
    "    else:\n",
    "        del df\n",
    "        df = pd.DataFrame()\n",
    "    return df, num_col, bin_col, cat_col"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# for data in classification_dataset_names:\n",
    "#     data = fetch_data(data)\n",
    "#     print(data.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "['GAMETES_Epistasis_2_Way_20atts_0.1H_EDM_1_1',\n 'GAMETES_Epistasis_2_Way_20atts_0.4H_EDM_1_1',\n 'GAMETES_Epistasis_3_Way_20atts_0.2H_EDM_1_1',\n 'GAMETES_Heterogeneity_20atts_1600_Het_0.4_0.2_50_EDM_2_001',\n 'GAMETES_Heterogeneity_20atts_1600_Het_0.4_0.2_75_EDM_2_001',\n 'Hill_Valley_with_noise',\n 'Hill_Valley_without_noise',\n 'adult',\n 'agaricus_lepiota']"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_dataset_names"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def experimentation(classification_dataset):\n",
    "    iters=5\n",
    "    res_rul = {}\n",
    "    sc = StandardScaler()\n",
    "    names = ['Reg-CART','CART','ORT','OCT','ORT-H','OCT-H','ORT+ORT-H','OCT+OCT-H']\n",
    "    df = fetch_data(classification_dataset)\n",
    "    if df.shape[0] > 50000:\n",
    "        return\n",
    "    if df.shape[1] > 100:\n",
    "        return\n",
    "    df, num_col, bin_col, cat_col = corr_data_for(df)\n",
    "    if df.empty:\n",
    "        return\n",
    "    y = df['target']\n",
    "    X = df.loc[:, df.columns != 'target']\n",
    "    #performance_by_iter = pd.DataFrame(columns = [\"Logistic Regression\", \"CART_rules\", \"OCT_rules\", \"OCTH_rules\", \"CART_rules_and_features\", \"OCT_rules_and_features\", \"OCTH_rules_and_features\"], index = np.arange(0, iters))\n",
    "    print(color.BOLD + '\\n\\n    ----------------------------------------- {} -----------------------------------------'.format(classification_dataset) + color.END)\n",
    "    rows_data, columns_data = X.shape\n",
    "    print('Dataset Information')\n",
    "    print('Rows:',rows_data,)\n",
    "    print('Columns:',columns_data)\n",
    "    print('Number of classes:',y.nunique())\n",
    "    print('Continous columns:', len(num_col))\n",
    "    print('Binary columns:', len(bin_col))\n",
    "    print('Categorical columns:',len(cat_col))\n",
    "    print('-------------------------------------------------')\n",
    "    for it in range(iters):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = it, stratify=y)\n",
    "        X_col = X_train.columns\n",
    "        col_len = len(X_col)\n",
    "        X_test.name = \"X_test\"\n",
    "        X_train.name = \"X_train\"\n",
    "        X_train = sc.fit_transform(X_train)\n",
    "        X_test = sc.transform(X_test)\n",
    "        X_train = pd.DataFrame(X_train,columns=X_col)\n",
    "        X_test = pd.DataFrame(X_test,columns=X_col)\n",
    "        factors = [round(col_len*0.5),col_len,round(col_len*1.2),round(col_len*1.4),round(col_len*1.6),round(col_len*1.8),round(col_len*2),round(col_len*2.5),round(col_len*3)]\n",
    "        factors_name = [0.5,1,1.2,1.4,1.6,1.8,2,2.5,3]\n",
    "\n",
    "\n",
    "\n",
    "        models, performance = generate_tree(X_train, y_train, X_test, y_test, n_num=1, feat_size=len(X.columns),  max_iter_hy=2,depth_grid=range(1,7), depth_grid_hy=range(1,3), complexity_bi = 0.001, complexity_hy=0.001,  Reg_CART=False, ORT=False, ORT_H=False, Clas_CART=True, OCT=True, OCT_H=True)\n",
    "        for perf,name in zip(performance,names):\n",
    "            if not not perf:\n",
    "                res_rul[(classification_dataset,name,it,1)] = sum(perf) / len(perf)\n",
    "\n",
    "        act_name = []\n",
    "        act_rules = []\n",
    "        for model,name in zip(models,names):\n",
    "            if not not model:\n",
    "                act_name += [name]\n",
    "                act_rules += [model]\n",
    "\n",
    "        datasets = gen_train_and_test_features(act_rules ,act_name , X_train, X_test)\n",
    "        for model in datasets.keys():\n",
    "            print(model)\n",
    "            X_train_rules_and_features, X_test_rules_and_features = datasets[model][0]\n",
    "            X_train_only_rules, X_test_only_rules = datasets[model][1]\n",
    "            print(len(X_train_rules_and_features.columns))\n",
    "            for len_c,fac_name in zip(factors,factors_name):\n",
    "                if len_c > len(X_train_only_rules.columns):\n",
    "                    min_len = len(X_train_only_rules.columns)\n",
    "                    min_name = 1\n",
    "                else:\n",
    "                    min_len = len_c\n",
    "                    min_name = fac_name\n",
    "                if (len_c <= X_train.shape[0]) & (len_c <= len(X_train_rules_and_features.columns)):\n",
    "                    cols = SelectKBest(k=len_c).fit(X_train_rules_and_features,y_train).get_feature_names_out()\n",
    "                    X_train_rules_features = X_train_rules_and_features[cols]\n",
    "                    X_test_rules_features = X_test_rules_and_features[cols]\n",
    "\n",
    "                    cols_1 = SelectKBest(k=min_len).fit(X_train_only_rules,y_train).get_feature_names_out()\n",
    "                    X_train_rules = X_train_only_rules[cols_1]\n",
    "                    X_test_rules = X_test_only_rules[cols_1]\n",
    "\n",
    "                    only_rules_acc = log_regression_pipeline(X_train_rules, X_test_rules, y_train, y_test)\n",
    "                    rules_and_features_acc = log_regression_pipeline(X_train_rules_features, X_test_rules_features, y_train, y_test)\n",
    "                    res_rul[(classification_dataset,model + \"_LG_rules\",it,min_name)] = only_rules_acc\n",
    "                    res_rul[(classification_dataset,model + \"_LG_rules_and_features\",it,fac_name)] = rules_and_features_acc\n",
    "\n",
    "                    only_rules_acc_SVM = SVM_pipeline(X_train_rules, X_test_rules, y_train, y_test)\n",
    "                    rules_and_features_acc_SVM = SVM_pipeline(X_train_rules_features, X_test_rules_features, y_train, y_test)\n",
    "                    res_rul[(classification_dataset,model + \"_SVM_rules\",it,min_name)] = only_rules_acc_SVM\n",
    "                    res_rul[(classification_dataset,model + \"_SVM_rules_and_features\",it,fac_name)] = rules_and_features_acc_SVM\n",
    "\n",
    "                    only_rules_acc_NB = NB_pipeline(X_train_rules, X_test_rules, y_train, y_test)\n",
    "                    rules_and_features_acc_NB = NB_pipeline(X_train_rules_features, X_test_rules_features, y_train, y_test)\n",
    "                    res_rul[(classification_dataset,model + \"_NB_rules\",it,min_name)] = only_rules_acc_NB\n",
    "                    res_rul[(classification_dataset,model + \"_NB_rules_and_features\",it,fac_name)] = rules_and_features_acc_NB\n",
    "\n",
    "                    only_rules_acc_KNN = KNN_pipeline(X_train_rules, X_test_rules, y_train, y_test)\n",
    "                    rules_and_features_acc_KNN = KNN_pipeline(X_train_rules_features, X_test_rules_features, y_train, y_test)\n",
    "                    res_rul[(classification_dataset,model + \"_KNN_rules\",it,min_name)] = only_rules_acc_KNN\n",
    "                    res_rul[(classification_dataset,model + \"_KNN_rules_and_features\",it,fac_name)] = rules_and_features_acc_KNN\n",
    "                else:\n",
    "                     continue\n",
    "\n",
    "        res_rul[(classification_dataset,'Logistic_Regression',it,1)] = log_regression_pipeline(X_train, X_test, y_train, y_test)\n",
    "\n",
    "        res_rul[(classification_dataset,\"Support Vector Machine\",it,1)] = SVM_pipeline(X_train, X_test, y_train, y_test)\n",
    "\n",
    "        res_rul[(classification_dataset,\"Naive Bayes\",it,1)] = NB_pipeline(X_train, X_test, y_train, y_test)\n",
    "\n",
    "        res_rul[(classification_dataset,\"K-Nearest-Neighbor\",it,1)] = KNN_pipeline(X_train, X_test, y_train, y_test)\n",
    "        with open('filename.pickle', 'wb') as handle:\n",
    "            pickle.dump(res_rul, handle)\n",
    "    return res_rul"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 2/9 [00:15<00:54,  7.74s/it]"
     ]
    }
   ],
   "source": [
    "from joblib import delayed\n",
    "from tqdm import tqdm\n",
    "res_rul = ProgressParallel(n_jobs=10)(delayed(experimentation)(data) for data in classification_dataset_names)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "b"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "res_rul = b"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "result = {}\n",
    "for d in res_rul:\n",
    "    result.update(d)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "k = pd.DataFrame(result,index=[0])\n",
    "k = k.stack(level=2).sort_index()\n",
    "k = k.stack(level=2).sort_index()\n",
    "k = k.swaplevel(axis=1)\n",
    "k = k.droplevel(0)\n",
    "t=k.mean(level=0,axis=1)\n",
    "t = t.mean(axis=0)\n",
    "t.sort_values(ascending = False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "k"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y = k.swaplevel(axis=1)\n",
    "y = y.var(level=0,axis=1)\n",
    "y = y.mean(axis=0)\n",
    "good_tests = y[y < 0.01].index\n",
    "good = list(good_tests)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "vaild_results = k.iloc[:,k.columns.isin(good, level=1)]\n",
    "vaild_results=vaild_results.mean(level=0,axis=1)\n",
    "vaild_results.mean(axis=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open('filename.pickle', 'rb') as handle:\n",
    "    b = pickle.load(handle)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "classification_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "b"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "classification_dataset = classification_dataset_names[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "iters=1\n",
    "res_rul = {}\n",
    "sc = StandardScaler()\n",
    "names = ['Reg-CART','CART','ORT','OCT','ORT-H','OCT-H','ORT+ORT-H','OCT+OCT-H']\n",
    "df = fetch_data(classification_dataset)\n",
    "\n",
    "df, num_col, bin_col, cat_col = corr_data_for(df)\n",
    "\n",
    "y = df['target']\n",
    "X = df.loc[:, df.columns != 'target']\n",
    "#performance_by_iter = pd.DataFrame(columns = [\"Logistic Regression\", \"CART_rules\", \"OCT_rules\", \"OCTH_rules\", \"CART_rules_and_features\", \"OCT_rules_and_features\", \"OCTH_rules_and_features\"], index = np.arange(0, iters))\n",
    "print(color.BOLD + '\\n\\n    ----------------------------------------- {} -----------------------------------------'.format(classification_dataset) + color.END)\n",
    "rows_data, columns_data = X.shape\n",
    "print('Dataset Information')\n",
    "print('Rows:',rows_data,)\n",
    "print('Columns:',columns_data)\n",
    "print('Number of classes:',y.nunique())\n",
    "print('Continous columns:', len(num_col))\n",
    "print('Binary columns:', len(bin_col))\n",
    "print('Categorical columns:',len(cat_col))\n",
    "print('-------------------------------------------------')\n",
    "for it in range(iters):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = it, stratify=y)\n",
    "    X_col = X_train.columns\n",
    "    col_len = len(X_col)\n",
    "    X_test.name = \"X_test\"\n",
    "    X_train.name = \"X_train\"\n",
    "    X_train = sc.fit_transform(X_train)\n",
    "    X_test = sc.transform(X_test)\n",
    "    X_train = pd.DataFrame(X_train,columns=X_col)\n",
    "    X_test = pd.DataFrame(X_test,columns=X_col)\n",
    "    factors = [round(col_len*0.5),col_len,round(col_len*1.2),round(col_len*1.4),round(col_len*1.6),round(col_len*1.8),round(col_len*2),round(col_len*2.5),round(col_len*3)]\n",
    "    factors_name = [0.5,1,1.2,1.4,1.6,1.8,2,2.5,3]\n",
    "\n",
    "\n",
    "\n",
    "    models, performance = generate_tree(X_train, y_train, X_test, y_test, n_num=1, feat_size=len(X.columns),  max_iter_hy=2,depth_grid=range(1,5), depth_grid_hy=range(1,3), complexity_bi = 0.001, complexity_hy=0.001,  Reg_CART=False, ORT=False, ORT_H=False, Clas_CART=True, OCT=True, OCT_H=False)\n",
    "    for perf,name in zip(performance,names):\n",
    "        if not not perf:\n",
    "            res_rul[(classification_dataset,name,it,1)] = sum(perf) / len(perf)\n",
    "\n",
    "    act_name = []\n",
    "    act_rules = []\n",
    "    for model,name in zip(models,names):\n",
    "        if not not model:\n",
    "            act_name += [name]\n",
    "            act_rules += [model]\n",
    "\n",
    "    datasets = gen_train_and_test_features(act_rules ,act_name , X_train, X_test)\n",
    "    for model in datasets.keys():\n",
    "        print(model)\n",
    "        X_train_rules_and_features, X_test_rules_and_features = datasets[model][0]\n",
    "        X_train_only_rules, X_test_only_rules = datasets[model][1]\n",
    "        print(len(X_train_rules_and_features.columns))\n",
    "        for len_c,fac_name in zip(factors,factors_name):\n",
    "            if len_c > len(X_train_only_rules.columns):\n",
    "                min_len = len(X_train_only_rules.columns)\n",
    "                min_name = 1\n",
    "            else:\n",
    "                min_len = len_c\n",
    "                min_name = fac_name\n",
    "            if (len_c <= X_train.shape[0]) & (len_c <= len(X_train_rules_and_features.columns)):\n",
    "                cols = SelectKBest(k=len_c).fit(X_train_rules_and_features,y_train).get_feature_names_out()\n",
    "                X_train_rules_features = X_train_rules_and_features[cols]\n",
    "                X_test_rules_features = X_test_rules_and_features[cols]\n",
    "\n",
    "                cols_1 = SelectKBest(k=min_len).fit(X_train_only_rules,y_train).get_feature_names_out()\n",
    "                X_train_rules = X_train_only_rules[cols_1]\n",
    "                X_test_rules = X_test_only_rules[cols_1]\n",
    "\n",
    "                only_rules_acc = log_regression_pipeline(X_train_rules, X_test_rules, y_train, y_test)\n",
    "                rules_and_features_acc = log_regression_pipeline(X_train_rules_features, X_test_rules_features, y_train, y_test)\n",
    "                res_rul[(classification_dataset,model + \"_LG_rules\",it,min_name)] = only_rules_acc\n",
    "                res_rul[(classification_dataset,model + \"_LG_rules_and_features\",it,fac_name)] = rules_and_features_acc\n",
    "\n",
    "                only_rules_acc_SVM = SVM_pipeline(X_train_rules, X_test_rules, y_train, y_test)\n",
    "                rules_and_features_acc_SVM = SVM_pipeline(X_train_rules_features, X_test_rules_features, y_train, y_test)\n",
    "                res_rul[(classification_dataset,model + \"_SVM_rules\",it,min_name)] = only_rules_acc_SVM\n",
    "                res_rul[(classification_dataset,model + \"_SVM_rules_and_features\",it,fac_name)] = rules_and_features_acc_SVM\n",
    "\n",
    "                only_rules_acc_NB = NB_pipeline(X_train_rules, X_test_rules, y_train, y_test)\n",
    "                rules_and_features_acc_NB = NB_pipeline(X_train_rules_features, X_test_rules_features, y_train, y_test)\n",
    "                res_rul[(classification_dataset,model + \"_NB_rules\",it,min_name)] = only_rules_acc_NB\n",
    "                res_rul[(classification_dataset,model + \"_NB_rules_and_features\",it,fac_name)] = rules_and_features_acc_NB\n",
    "\n",
    "                only_rules_acc_KNN = KNN_pipeline(X_train_rules, X_test_rules, y_train, y_test)\n",
    "                rules_and_features_acc_KNN = KNN_pipeline(X_train_rules_features, X_test_rules_features, y_train, y_test)\n",
    "                res_rul[(classification_dataset,model + \"_KNN_rules\",it,min_name)] = only_rules_acc_KNN\n",
    "                res_rul[(classification_dataset,model + \"_KNN_rules_and_features\",it,fac_name)] = rules_and_features_acc_KNN\n",
    "            else:\n",
    "                 continue\n",
    "\n",
    "    res_rul[(classification_dataset,'Logistic_Regression',it,1)] = log_regression_pipeline(X_train, X_test, y_train, y_test)\n",
    "\n",
    "    res_rul[(classification_dataset,\"Support Vector Machine\",it,1)] = SVM_pipeline(X_train, X_test, y_train, y_test)\n",
    "\n",
    "    res_rul[(classification_dataset,\"Naive Bayes\",it,1)] = NB_pipeline(X_train, X_test, y_train, y_test)\n",
    "\n",
    "    res_rul[(classification_dataset,\"K-Nearest-Neighbor\",it,1)] = KNN_pipeline(X_train, X_test, y_train, y_test)\n",
    "    with open('filename.pickle', 'wb') as handle:\n",
    "        pickle.dump(res_rul, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cols = SelectKBest(k=13).fit(X_train_rules_and_features,y_train).get_feature_names_out()\n",
    "X_train_rules_features = X_train_rules_and_features[cols]\n",
    "X_test_rules_features = X_test_rules_and_features[cols]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "k = pd.DataFrame(res_rul, index=[0])\n",
    "k = k.stack(level=2).sort_index()\n",
    "k = k.stack(level=2).sort_index()\n",
    "k"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train_only_rules"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "result = {}\n",
    "for d in res_rul:\n",
    "    result.update(d)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "res_rul"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "round(col_len*1.5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "col_len"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "result = {}\n",
    "for d in res_rul:\n",
    "    result.update(d)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "k = pd.DataFrame(res_rul, index=[0])\n",
    "k = k.stack(level=2).sort_index()\n",
    "k = k.stack(level=2).sort_index()\n",
    "k"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "[round(col_len*0.5),col_len,round(col_len*1.25),round(col_len*1.5),round(col_len*2)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "datasets.keys()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.models import Sequential\n",
    "import keras.utils\n",
    "import keras_tuner\n",
    "#from tensorflow import keras\n",
    "from keras import utils as np_utils\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def NN_creator(hp):\n",
    "  model = Sequential()\n",
    "  model.add(Dense(30, activation='relu', input_dim=30))\n",
    "\n",
    "  # Tune the number of dense layers\n",
    "  for i in range(hp.Int('num_layers', 1, 5)):\n",
    "\n",
    "    # Tune the number of units in the each dense layer\n",
    "    hp_units = hp.Int('units_'+str(i), min_value=3, max_value=18,step=1)\n",
    "    model.add(keras.layers.Dense(units=hp_units, activation='relu'))\n",
    "\n",
    "    # Tune the dropout rate in the each dense layer\n",
    "    hp_dropout = hp.Float('rate', min_value=0.0, max_value=0.5, step=0.1)\n",
    "    model.add(keras.layers.Dropout(hp_dropout))\n",
    "\n",
    "  # Add dense output layer\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "  # Tune the learning rate for the optimizer\n",
    "  hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "  model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
    "                loss='binary_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "  return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a935b482",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(nrows = 5, ncols = 4, gridspec_kw = {\"hspace\": 0.25})\n",
    "import seaborn as sns\n",
    "fig.set_size_inches(30, 25)\n",
    "iteration = 0\n",
    "\n",
    "for m in range(5):\n",
    "    for j in range(4):\n",
    "\n",
    "        dataset = classification_dataset_names[:20][iteration]\n",
    "\n",
    "        columns = [i for i in k.columns if dataset in i]\n",
    "        sns.boxplot(k[columns], ax = ax[m, j])\n",
    "\n",
    "        ax[m, j].set_title(dataset)\n",
    "\n",
    "        ax[m, j].set_xticklabels(['CART Rules', \"OCT Rules\", \"Logistic Regression\", \"RuleFit\", \"ORRFA\"])\n",
    "\n",
    "        iteration += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2bb77d",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "CART_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e4f4f4",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.violinplot(data=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27cefcb",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d06e2f",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "del performance_by_iter['OCTH_rules']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a166560",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "del performance_by_iter['OCTH_rules_and_features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e2c960",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "df.loc[eval(rule)].index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc560a0",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "df = X_train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb79589",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "rule = rule.replace(\"feature\", \"df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff36319",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ab8c83",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "loc[eval(rule)].index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a5efa5",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "rule = rules[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8af6003",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "for i, rules in enumerate(act_rules):\n",
    "    print(i)\n",
    "    print(rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e6a767",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "act_rules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746991b1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f58967",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "for i, rules in enumerate(act_rules):\n",
    "    print(i)\n",
    "    print(rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30830c6b",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "performance_by_iter.rename(columns = {column: column.replace(\"OCT_rules_and_features\", \"ORRFA\")}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94a02e4",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "fig, ax = plt.subplots()\n",
    "sns.boxplot(data = performance_by_iter)\n",
    "fig.set_size_inches(20, 10)\n",
    "ax.set_xticklabels(performance_by_iter.columns.values)\n",
    "# ax.set_ylim(0.93, 0.995)\n",
    "ax.tick_params(rotation = 0, labelsize = 14)\n",
    "ax.set_ylabel(\"Accuracy\", fontsize = 14)\n",
    "ax.set_title(\"Accuracy of Logistic Regression, RuleFit and ORRFA\", fontsize = 15)\n",
    "# ax.set_ylabel()\n",
    "plt.savefig('Benchmark ORRFA.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91219303",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5682f5e6",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "performance_by_iter.mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
